# ğŸ“¡ RFID-RAG  
AI-Powered Retrieval-Augmented Generation System for RFID Knowledge

RFID-RAG adalah sistem **AI Question Answering berbasis Retrieval-Augmented Generation (RAG)** yang dirancang khusus untuk topik **RFID (Radio Frequency Identification)**.

Sistem ini menggabungkan **semantic vector search**, **knowledge base lokal**, **Large Language Model (LLM)**, dan **online search fallback** untuk menghasilkan jawaban yang **akurat, kontekstual, dan natural**, bahkan ketika pengetahuan lokal belum lengkap.

---

## âœ¨ Features

- ğŸ” **Semantic Vector Search** (berbasis makna, bukan keyword)
- ğŸ§  **Hybrid RAG Architecture** (Local KB + Online Search + LLM)
- ğŸŒ **Online Search Fallback** saat confidence rendah
- ğŸ—‚ï¸ **Human-in-the-loop Knowledge Approval**
- ğŸ“Š **Confidence Score** untuk setiap jawaban
- ğŸ”Œ **API-first Design** (Web, WhatsApp, Telegram, n8n)
- ğŸ’» **CPU-only** (tanpa GPU)

---

## ğŸ—ï¸ System Architecture

```
User Question
      â†“
Vector Search (Local Knowledge Base)
      â†“
Confidence Evaluation
      â”œâ”€â”€ High Confidence  â†’ Local RAG Answer
      â””â”€â”€ Low Confidence   â†’ Online Search â†’ LLM Answer
      â†“
(Optional) Save to Pending Knowledge Base
```

---

## ğŸ“ Project Structure

```
rfid-rag/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ rag.py
â”œâ”€â”€ vectorstore.py
â”œâ”€â”€ online_search.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data.json
â”‚   â””â”€â”€ data_pending.json
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Tech Stack

| Layer | Technology |
|-----|-----------|
| API | FastAPI |
| LLM | Mistral (via Ollama) |
| Embeddings | Sentence Transformers |
| Vector Search | Cosine Similarity |
| Online Search | DuckDuckGo / Custom Search API |
| Storage | JSON |
| Deployment | CPU-only |

---

## ğŸš€ Getting Started (Conda)

### 1. Create Environment
```bash
conda create -n rfid-rag python=3.10 -y
conda activate rfid-rag
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Install & Run Ollama
```bash
ollama pull mistral
ollama serve
```

Ollama runs at:
```
http://localhost:11434
```

### 4. Run API
```bash
uvicorn app:app --reload
```

API available at:
```
http://localhost:8000
```

---

## ğŸ§  Knowledge Workflow

1. User asks a question
2. Semantic vector search is performed
3. Confidence score is evaluated
4. If low confidence:
   - Online search is triggered
   - LLM generates answer
5. Online knowledge saved to:
```bash
data/data_pending.json
```
6. Approved knowledge stored in:
```bash
data/data.json
```
